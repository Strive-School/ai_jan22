{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1888846f220>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1888846fe80>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x188881d79e0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x188882d5380>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1888850f300>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x188881d7970>)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Hello world. My name is John Moskowitz. I am learning Python 1 5. \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(document)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n",
      "<class 'spacy.tokens.token.Token'>\n"
     ]
    }
   ],
   "source": [
    "for i in doc1:\n",
    "    print(type(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first token of the document\n",
    "# A TOKEN CAN BE A WORD & PUNCTUTATION\n",
    "doc1[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on English in module spacy.lang.en object:\n",
      "\n",
      "class English(spacy.language.Language)\n",
      " |  English(vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Optional[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]]] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      English\n",
      " |      spacy.language.Language\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Defaults = <class 'spacy.lang.en.EnglishDefaults'>\n",
      " |  \n",
      " |  default_config = {'paths': {'train': None, 'dev': None, 'vectors'...s'...\n",
      " |  \n",
      " |  factories = {'attribute_ruler': <function make_attribute_rul...<functi...\n",
      " |  \n",
      " |  lang = 'en'\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __call__(self, text: Union[str, spacy.tokens.doc.Doc], *, disable: Iterable[str] = [], component_cfg: Optional[Dict[str, Dict[str, Any]]] = None) -> spacy.tokens.doc.Doc\n",
      " |      Apply the pipeline to some text. The text can span multiple sentences,\n",
      " |      and can contain arbitrary whitespace. Alignment into the original string\n",
      " |      is preserved.\n",
      " |      \n",
      " |      text (Union[str, Doc]): If `str`, the text to be processed. If `Doc`,\n",
      " |          the doc will be passed directly to the pipeline, skipping\n",
      " |          `Language.make_doc`.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, dict]): An optional dictionary with extra\n",
      " |          keyword arguments for specific components.\n",
      " |      RETURNS (Doc): A container for accessing the annotations.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#call\n",
      " |  \n",
      " |  __init__(self, vocab: Union[spacy.vocab.Vocab, bool] = True, *, max_length: int = 1000000, meta: Dict[str, Any] = {}, create_tokenizer: Optional[Callable[[ForwardRef('Language')], Callable[[str], spacy.tokens.doc.Doc]]] = None, batch_size: int = 1000, **kwargs) -> None\n",
      " |      Initialise a Language object.\n",
      " |      \n",
      " |      vocab (Vocab): A `Vocab` object. If `True`, a vocab is created.\n",
      " |      meta (dict): Custom meta data for the Language class. Is written to by\n",
      " |          models to add model meta data.\n",
      " |      max_length (int): Maximum number of characters in a single text. The\n",
      " |          current models may run out memory on extremely long texts, due to\n",
      " |          large internal allocations. You should segment these texts into\n",
      " |          meaningful units, e.g. paragraphs, subsections etc, before passing\n",
      " |          them to spaCy. Default maximum length is 1,000,000 charas (1mb). As\n",
      " |          a rule of thumb, if all pipeline components are enabled, spaCy's\n",
      " |          default models currently requires roughly 1GB of temporary memory per\n",
      " |          100,000 characters in one text.\n",
      " |      create_tokenizer (Callable): Function that takes the nlp object and\n",
      " |          returns a tokenizer.\n",
      " |      batch_size (int): Default batch size for pipe and evaluate.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#init\n",
      " |  \n",
      " |  add_pipe(self, factory_name: str, name: Optional[str] = None, *, before: Union[str, int, NoneType] = None, after: Union[str, int, NoneType] = None, first: Optional[bool] = None, last: Optional[bool] = None, source: Optional[ForwardRef('Language')] = None, config: Dict[str, Any] = {}, raw_config: Optional[thinc.config.Config] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Add a component to the processing pipeline. Valid components are\n",
      " |      callables that take a `Doc` object, modify it and return it. Only one\n",
      " |      of before/after/first/last can be set. Default behaviour is \"last\".\n",
      " |      \n",
      " |      factory_name (str): Name of the component factory.\n",
      " |      name (str): Name of pipeline component. Overwrites existing\n",
      " |          component.name attribute if available. If no name is set and\n",
      " |          the component exposes no name attribute, component.__name__ is\n",
      " |          used. An error is raised if a name already exists in the pipeline.\n",
      " |      before (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly before.\n",
      " |      after (Union[str, int]): Name or index of the component to insert new\n",
      " |          component directly after.\n",
      " |      first (bool): If True, insert component first in the pipeline.\n",
      " |      last (bool): If True, insert component last in the pipeline.\n",
      " |      source (Language): Optional loaded nlp object to copy the pipeline\n",
      " |          component from.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#add_pipe\n",
      " |  \n",
      " |  analyze_pipes(self, *, keys: List[str] = ['assigns', 'requires', 'scores', 'retokenizes'], pretty: bool = False) -> Optional[Dict[str, Any]]\n",
      " |      Analyze the current pipeline components, print a summary of what\n",
      " |      they assign or require and check that all requirements are met.\n",
      " |      \n",
      " |      keys (List[str]): The meta values to display in the table. Corresponds\n",
      " |          to values in FactoryMeta, defined by @Language.factory decorator.\n",
      " |      pretty (bool): Pretty-print the results.\n",
      " |      RETURNS (dict): The data.\n",
      " |  \n",
      " |  begin_training(self, get_examples: Optional[Callable[[], Iterable[spacy.training.example.Example]]] = None, *, sgd: Optional[thinc.optimizers.Optimizer] = None) -> thinc.optimizers.Optimizer\n",
      " |  \n",
      " |  create_optimizer(self)\n",
      " |      Create an optimizer, usually using the [training.optimizer] config.\n",
      " |  \n",
      " |  create_pipe(self, factory_name: str, name: Optional[str] = None, *, config: Dict[str, Any] = {}, raw_config: Optional[thinc.config.Config] = None, validate: bool = True) -> 'Pipe'\n",
      " |      Create a pipeline component. Mostly used internally. To create and\n",
      " |      add a component to the pipeline, you can use nlp.add_pipe.\n",
      " |      \n",
      " |      factory_name (str): Name of component factory.\n",
      " |      name (Optional[str]): Optional name to assign to component instance.\n",
      " |          Defaults to factory name if not set.\n",
      " |      config (Dict[str, Any]): Config parameters to use for this component.\n",
      " |          Will be merged with default config, if available.\n",
      " |      raw_config (Optional[Config]): Internals: the non-interpolated config.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#create_pipe\n",
      " |  \n",
      " |  create_pipe_from_source(self, source_name: str, source: 'Language', *, name: str) -> Tuple[ForwardRef('Pipe'), str]\n",
      " |      Create a pipeline component by copying it from an existing model.\n",
      " |      \n",
      " |      source_name (str): Name of the component in the source pipeline.\n",
      " |      source (Language): The source nlp object to copy from.\n",
      " |      name (str): Optional alternative name to use in current pipeline.\n",
      " |      RETURNS (Tuple[Callable, str]): The component and its factory name.\n",
      " |  \n",
      " |  disable_pipe(self, name: str) -> None\n",
      " |      Disable a pipeline component. The component will still exist on\n",
      " |      the nlp object, but it won't be run as part of the pipeline. Does\n",
      " |      nothing if the component is already disabled.\n",
      " |      \n",
      " |      name (str): The name of the component to disable.\n",
      " |  \n",
      " |  disable_pipes(self, *names) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      This method has been deprecated since 3.0\n",
      " |  \n",
      " |  enable_pipe(self, name: str) -> None\n",
      " |      Enable a previously disabled pipeline component so it's run as part\n",
      " |      of the pipeline. Does nothing if the component is already enabled.\n",
      " |      \n",
      " |      name (str): The name of the component to enable.\n",
      " |  \n",
      " |  evaluate(self, examples: Iterable[spacy.training.example.Example], *, batch_size: Optional[int] = None, scorer: Optional[spacy.scorer.Scorer] = None, component_cfg: Optional[Dict[str, Dict[str, Any]]] = None, scorer_cfg: Optional[Dict[str, Any]] = None) -> Dict[str, Any]\n",
      " |      Evaluate a model's pipeline components.\n",
      " |      \n",
      " |      examples (Iterable[Example]): `Example` objects.\n",
      " |      batch_size (Optional[int]): Batch size to use.\n",
      " |      scorer (Optional[Scorer]): Scorer to use. If not passed in, a new one\n",
      " |          will be created.\n",
      " |      component_cfg (dict): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      scorer_cfg (dict): An optional dictionary with extra keyword arguments\n",
      " |          for the scorer.\n",
      " |      \n",
      " |      RETURNS (Scorer): The scorer containing the evaluation results.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#evaluate\n",
      " |  \n",
      " |  from_bytes(self, bytes_data: bytes, *, exclude: Iterable[str] = []) -> 'Language'\n",
      " |      Load state from a binary string.\n",
      " |      \n",
      " |      bytes_data (bytes): The data to load from.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_bytes\n",
      " |  \n",
      " |  from_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = [], overrides: Dict[str, Any] = {}) -> 'Language'\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |      returns it. If the saved `Language` object contains a model, the\n",
      " |      model will be loaded.\n",
      " |      \n",
      " |      path (str / Path): A path to a directory.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (Language): The modified `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_disk\n",
      " |  \n",
      " |  get_pipe(self, name: str) -> 'Pipe'\n",
      " |      Get a pipeline component for a given component name.\n",
      " |      \n",
      " |      name (str): Name of pipeline component to get.\n",
      " |      RETURNS (callable): The pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#get_pipe\n",
      " |  \n",
      " |  get_pipe_config(self, name: str) -> thinc.config.Config\n",
      " |      Get the config used to create a pipeline component.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (Config): The config used to create the pipeline component.\n",
      " |  \n",
      " |  get_pipe_meta(self, name: str) -> 'FactoryMeta'\n",
      " |      Get the meta information for a given component name.\n",
      " |      \n",
      " |      name (str): The component name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given component name.\n",
      " |  \n",
      " |  has_pipe(self, name: str) -> bool\n",
      " |      Check if a component name is present in the pipeline. Equivalent to\n",
      " |      `name in nlp.pipe_names`.\n",
      " |      \n",
      " |      name (str): Name of the component.\n",
      " |      RETURNS (bool): Whether a component of the name exists in the pipeline.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#has_pipe\n",
      " |  \n",
      " |  initialize(self, get_examples: Optional[Callable[[], Iterable[spacy.training.example.Example]]] = None, *, sgd: Optional[thinc.optimizers.Optimizer] = None) -> thinc.optimizers.Optimizer\n",
      " |      Initialize the pipe for training, using data examples if available.\n",
      " |      \n",
      " |      get_examples (Callable[[], Iterable[Example]]): Optional function that\n",
      " |          returns gold-standard Example objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer to use for updates. If not\n",
      " |          provided, will be created using the .create_optimizer() method.\n",
      " |      RETURNS (thinc.api.Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#initialize\n",
      " |  \n",
      " |  make_doc(self, text: str) -> spacy.tokens.doc.Doc\n",
      " |      Turn a text into a Doc object.\n",
      " |      \n",
      " |      text (str): The text to process.\n",
      " |      RETURNS (Doc): The processed doc.\n",
      " |  \n",
      " |  pipe(self, texts: Union[Iterable[Union[str, spacy.tokens.doc.Doc]], Iterable[Tuple[Union[str, spacy.tokens.doc.Doc], ~_AnyContext]]], *, as_tuples: bool = False, batch_size: Optional[int] = None, disable: Iterable[str] = [], component_cfg: Optional[Dict[str, Dict[str, Any]]] = None, n_process: int = 1) -> Union[Iterator[spacy.tokens.doc.Doc], Iterator[Tuple[spacy.tokens.doc.Doc, ~_AnyContext]]]\n",
      " |      Process texts as a stream, and yield `Doc` objects in order.\n",
      " |      \n",
      " |      texts (Iterable[Union[str, Doc]]): A sequence of texts or docs to\n",
      " |          process.\n",
      " |      as_tuples (bool): If set to True, inputs should be a sequence of\n",
      " |          (text, context) tuples. Output will then be a sequence of\n",
      " |          (doc, context) tuples. Defaults to False.\n",
      " |      batch_size (Optional[int]): The number of texts to buffer.\n",
      " |      disable (List[str]): Names of the pipeline components to disable.\n",
      " |      component_cfg (Dict[str, Dict]): An optional dictionary with extra keyword\n",
      " |          arguments for specific components.\n",
      " |      n_process (int): Number of processors to process texts. If -1, set `multiprocessing.cpu_count()`.\n",
      " |      YIELDS (Doc): Documents in the order of the original text.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#pipe\n",
      " |  \n",
      " |  rehearse(self, examples: Iterable[spacy.training.example.Example], *, sgd: Optional[thinc.optimizers.Optimizer] = None, losses: Optional[Dict[str, float]] = None, component_cfg: Optional[Dict[str, Dict[str, Any]]] = None, exclude: Iterable[str] = []) -> Dict[str, float]\n",
      " |      Make a \"rehearsal\" update to the models in the pipeline, to prevent\n",
      " |      forgetting. Rehearsal updates run an initial copy of the model over some\n",
      " |      data, and update the model so its current predictions are more like the\n",
      " |      initial ones. This is useful for keeping a pretrained model on-track,\n",
      " |      even if you're updating it with a smaller set of examples.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of `Example` objects.\n",
      " |      sgd (Optional[Optimizer]): An optimizer.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      RETURNS (dict): Results from the update.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> raw_text_batches = minibatch(raw_texts)\n",
      " |          >>> for labelled_batch in minibatch(examples):\n",
      " |          >>>     nlp.update(labelled_batch)\n",
      " |          >>>     raw_batch = [Example.from_dict(nlp.make_doc(text), {}) for text in next(raw_text_batches)]\n",
      " |          >>>     nlp.rehearse(raw_batch)\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rehearse\n",
      " |  \n",
      " |  remove_pipe(self, name: str) -> Tuple[str, ForwardRef('Pipe')]\n",
      " |      Remove a component from the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to remove.\n",
      " |      RETURNS (tuple): A `(name, component)` tuple of the removed component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#remove_pipe\n",
      " |  \n",
      " |  rename_pipe(self, old_name: str, new_name: str) -> None\n",
      " |      Rename a pipeline component.\n",
      " |      \n",
      " |      old_name (str): Name of the component to rename.\n",
      " |      new_name (str): New name of the component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#rename_pipe\n",
      " |  \n",
      " |  replace_listeners(self, tok2vec_name: str, pipe_name: str, listeners: Iterable[str]) -> None\n",
      " |      Find listener layers (connecting to a token-to-vector embedding\n",
      " |      component) of a given pipeline component model and replace\n",
      " |      them with a standalone copy of the token-to-vector layer. This can be\n",
      " |      useful when training a pipeline with components sourced from an existing\n",
      " |      pipeline: if multiple components (e.g. tagger, parser, NER) listen to\n",
      " |      the same tok2vec component, but some of them are frozen and not updated,\n",
      " |      their performance may degrade significally as the tok2vec component is\n",
      " |      updated with new data. To prevent this, listeners can be replaced with\n",
      " |      a standalone tok2vec layer that is owned by the component and doesn't\n",
      " |      change if the component isn't updated.\n",
      " |      \n",
      " |      tok2vec_name (str): Name of the token-to-vector component, typically\n",
      " |          \"tok2vec\" or \"transformer\".\n",
      " |      pipe_name (str): Name of pipeline component to replace listeners for.\n",
      " |      listeners (Iterable[str]): The paths to the listeners, relative to the\n",
      " |          component config, e.g. [\"model.tok2vec\"]. Typically, implementations\n",
      " |          will only connect to one tok2vec component, [model.tok2vec], but in\n",
      " |          theory, custom models can use multiple listeners. The value here can\n",
      " |          either be an empty list to not replace any listeners, or a complete\n",
      " |          (!) list of the paths to all listener layers used by the model.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_listeners\n",
      " |  \n",
      " |  replace_pipe(self, name: str, factory_name: str, *, config: Dict[str, Any] = {}, validate: bool = True) -> 'Pipe'\n",
      " |      Replace a component in the pipeline.\n",
      " |      \n",
      " |      name (str): Name of the component to replace.\n",
      " |      factory_name (str): Factory name of replacement component.\n",
      " |      config (Optional[Dict[str, Any]]): Config parameters to use for this\n",
      " |          component. Will be merged with default config, if available.\n",
      " |      validate (bool): Whether to validate the component config against the\n",
      " |          arguments and types expected by the factory.\n",
      " |      RETURNS (Pipe): The new pipeline component.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#replace_pipe\n",
      " |  \n",
      " |  resume_training(self, *, sgd: Optional[thinc.optimizers.Optimizer] = None) -> thinc.optimizers.Optimizer\n",
      " |      Continue training a pretrained model.\n",
      " |      \n",
      " |      Create and return an optimizer, and initialize \"rehearsal\" for any pipeline\n",
      " |      component that has a .rehearse() method. Rehearsal is used to prevent\n",
      " |      models from \"forgetting\" their initialized \"knowledge\". To perform\n",
      " |      rehearsal, collect samples of text you want the models to retain performance\n",
      " |      on, and call nlp.rehearse() with a batch of Example objects.\n",
      " |      \n",
      " |      RETURNS (Optimizer): The optimizer.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#resume_training\n",
      " |  \n",
      " |  select_pipes(self, *, disable: Union[str, Iterable[str], NoneType] = None, enable: Union[str, Iterable[str], NoneType] = None) -> 'DisabledPipes'\n",
      " |      Disable one or more pipeline components. If used as a context\n",
      " |      manager, the pipeline will be restored to the initial state at the end\n",
      " |      of the block. Otherwise, a DisabledPipes object is returned, that has\n",
      " |      a `.restore()` method you can use to undo your changes.\n",
      " |      \n",
      " |      disable (str or iterable): The name(s) of the pipes to disable\n",
      " |      enable (str or iterable): The name(s) of the pipes to enable - all others will be disabled\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#select_pipes\n",
      " |  \n",
      " |  set_error_handler(self, error_handler: Callable[[str, ForwardRef('Pipe'), List[spacy.tokens.doc.Doc], Exception], NoReturn])\n",
      " |      Set an error handler object for all the components in the pipeline that implement\n",
      " |      a set_error_handler function.\n",
      " |      \n",
      " |      error_handler (Callable[[str, Pipe, List[Doc], Exception], NoReturn]):\n",
      " |          Function that deals with a failing batch of documents. This callable function should take in\n",
      " |          the component's name, the component itself, the offending batch of documents, and the exception\n",
      " |          that was thrown.\n",
      " |      DOCS: https://spacy.io/api/language#set_error_handler\n",
      " |  \n",
      " |  to_bytes(self, *, exclude: Iterable[str] = []) -> bytes\n",
      " |      Serialize the current state to a binary string.\n",
      " |      \n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      RETURNS (bytes): The serialized form of the `Language` object.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_bytes\n",
      " |  \n",
      " |  to_disk(self, path: Union[str, pathlib.Path], *, exclude: Iterable[str] = []) -> None\n",
      " |      Save the current state to a directory.  If a model is loaded, this\n",
      " |      will include the model.\n",
      " |      \n",
      " |      path (str / Path): Path to a directory, which will be created if\n",
      " |          it doesn't exist.\n",
      " |      exclude (Iterable[str]): Names of components or serialization fields to exclude.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#to_disk\n",
      " |  \n",
      " |  update(self, examples: Iterable[spacy.training.example.Example], _: Optional[Any] = None, *, drop: float = 0.0, sgd: Optional[thinc.optimizers.Optimizer] = None, losses: Optional[Dict[str, float]] = None, component_cfg: Optional[Dict[str, Dict[str, Any]]] = None, exclude: Iterable[str] = [], annotates: Iterable[str] = [])\n",
      " |      Update the models in the pipeline.\n",
      " |      \n",
      " |      examples (Iterable[Example]): A batch of examples\n",
      " |      _: Should not be set - serves to catch backwards-incompatible scripts.\n",
      " |      drop (float): The dropout rate.\n",
      " |      sgd (Optimizer): An optimizer.\n",
      " |      losses (Dict[str, float]): Dictionary to update with the loss, keyed by\n",
      " |          component.\n",
      " |      component_cfg (Dict[str, Dict]): Config parameters for specific pipeline\n",
      " |          components, keyed by component name.\n",
      " |      exclude (Iterable[str]): Names of components that shouldn't be updated.\n",
      " |      annotates (Iterable[str]): Names of components that should set\n",
      " |          annotations on the predicted examples after updating.\n",
      " |      RETURNS (Dict[str, float]): The updated losses dictionary\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#update\n",
      " |  \n",
      " |  use_params(self, params: Optional[dict])\n",
      " |      Replace weights of models in the pipeline with those provided in the\n",
      " |      params dictionary. Can be used as a contextmanager, in which case,\n",
      " |      models go back to their original weights after the block.\n",
      " |      \n",
      " |      params (dict): A dictionary of parameters keyed by model ID.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> with nlp.use_params(optimizer.averages):\n",
      " |          >>>     nlp.to_disk(\"/tmp/checkpoint\")\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#use_params\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __init_subclass__(**kwargs) from builtins.type\n",
      " |      This method is called when a class is subclassed.\n",
      " |      \n",
      " |      The default implementation does nothing. It may be\n",
      " |      overridden to extend subclasses.\n",
      " |  \n",
      " |  component(name: str, *, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, func: Optional[ForwardRef('Pipe')] = None) -> Callable[..., Any] from builtins.type\n",
      " |      Register a new pipeline component. Can be used for stateless function\n",
      " |      components that don't require a separate factory. Can be used as a\n",
      " |      decorator on a function or classmethod, or called as a function with the\n",
      " |      factory provided as the func keyword argument. To create a component and\n",
      " |      add it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#component\n",
      " |  \n",
      " |  factory(name: str, *, default_config: Dict[str, Any] = {}, assigns: Iterable[str] = [], requires: Iterable[str] = [], retokenizes: bool = False, default_score_weights: Dict[str, Optional[float]] = {}, func: Optional[Callable] = None) -> Callable from builtins.type\n",
      " |      Register a new pipeline component factory. Can be used as a decorator\n",
      " |      on a function or classmethod, or called as a function with the factory\n",
      " |      provided as the func keyword argument. To create a component and add\n",
      " |      it to the pipeline, you can use nlp.add_pipe(name).\n",
      " |      \n",
      " |      name (str): The name of the component factory.\n",
      " |      default_config (Dict[str, Any]): Default configuration, describing the\n",
      " |          default values of the factory arguments.\n",
      " |      assigns (Iterable[str]): Doc/Token attributes assigned by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      requires (Iterable[str]): Doc/Token attributes required by this component,\n",
      " |          e.g. \"token.ent_id\". Used for pipeline analysis.\n",
      " |      retokenizes (bool): Whether the component changes the tokenization.\n",
      " |          Used for pipeline analysis.\n",
      " |      default_score_weights (Dict[str, Optional[float]]): The scores to report during\n",
      " |          training, and their default weight towards the final score used to\n",
      " |          select the best model. Weights should sum to 1.0 per component and\n",
      " |          will be combined and normalized for the whole pipeline. If None,\n",
      " |          the score won't be shown in the logs or be weighted.\n",
      " |      func (Optional[Callable]): Factory function if not used as a decorator.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#factory\n",
      " |  \n",
      " |  from_config(config: Union[Dict[str, Any], thinc.config.Config] = {}, *, vocab: Union[spacy.vocab.Vocab, bool] = True, disable: Iterable[str] = [], exclude: Iterable[str] = [], meta: Dict[str, Any] = {}, auto_fill: bool = True, validate: bool = True) -> 'Language' from builtins.type\n",
      " |      Create the nlp object from a loaded config. Will set up the tokenizer\n",
      " |      and language data, add pipeline components etc. If no config is provided,\n",
      " |      the default config of the given language is used.\n",
      " |      \n",
      " |      config (Dict[str, Any] / Config): The loaded config.\n",
      " |      vocab (Vocab): A Vocab object. If True, a vocab is created.\n",
      " |      disable (Iterable[str]): Names of pipeline components to disable.\n",
      " |          Disabled pipes will be loaded but they won't be run unless you\n",
      " |          explicitly enable them by calling nlp.enable_pipe.\n",
      " |      exclude (Iterable[str]): Names of pipeline components to exclude.\n",
      " |          Excluded components won't be loaded.\n",
      " |      meta (Dict[str, Any]): Meta overrides for nlp.meta.\n",
      " |      auto_fill (bool): Automatically fill in missing values in config based\n",
      " |          on defaults and function argument annotations.\n",
      " |      validate (bool): Validate the component config and arguments against\n",
      " |          the types expected by the factory.\n",
      " |      RETURNS (Language): The initialized Language class.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#from_config\n",
      " |  \n",
      " |  get_factory_meta(name: str) -> 'FactoryMeta' from builtins.type\n",
      " |      Get the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      RETURNS (FactoryMeta): The meta for the given factory name.\n",
      " |  \n",
      " |  get_factory_name(name: str) -> str from builtins.type\n",
      " |      Get the internal factory name based on the language subclass.\n",
      " |      \n",
      " |      name (str): The factory name.\n",
      " |      RETURNS (str): The internal factory name.\n",
      " |  \n",
      " |  has_factory(name: str) -> bool from builtins.type\n",
      " |      RETURNS (bool): Whether a factory of that name is registered.\n",
      " |  \n",
      " |  set_factory_meta(name: str, value: 'FactoryMeta') -> None from builtins.type\n",
      " |      Set the meta information for a given factory name.\n",
      " |      \n",
      " |      name (str): The component factory name.\n",
      " |      value (FactoryMeta): The meta to set.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from spacy.language.Language:\n",
      " |  \n",
      " |  component_names\n",
      " |      Get the names of the available pipeline components. Includes all\n",
      " |      active and inactive pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  components\n",
      " |      Get all (name, component) tuples in the pipeline, including the\n",
      " |      currently disabled components.\n",
      " |  \n",
      " |  disabled\n",
      " |      Get the names of all disabled components.\n",
      " |      \n",
      " |      RETURNS (List[str]): The disabled components.\n",
      " |  \n",
      " |  factory_names\n",
      " |      Get names of all available factories.\n",
      " |      \n",
      " |      RETURNS (List[str]): The factory names.\n",
      " |  \n",
      " |  path\n",
      " |  \n",
      " |  pipe_factories\n",
      " |      Get the component factories for the available pipeline components.\n",
      " |      \n",
      " |      RETURNS (Dict[str, str]): Factory names, keyed by component names.\n",
      " |  \n",
      " |  pipe_labels\n",
      " |      Get the labels set by the pipeline components, if available (if\n",
      " |      the component exposes a labels property and the labels are not\n",
      " |      hidden).\n",
      " |      \n",
      " |      RETURNS (Dict[str, List[str]]): Labels keyed by component name.\n",
      " |  \n",
      " |  pipe_names\n",
      " |      Get names of available active pipeline components.\n",
      " |      \n",
      " |      RETURNS (List[str]): List of component name strings, in order.\n",
      " |  \n",
      " |  pipeline\n",
      " |      The processing pipeline consisting of (name, component) tuples. The\n",
      " |      components are called on the Doc in order as it passes through the\n",
      " |      pipeline.\n",
      " |      \n",
      " |      RETURNS (List[Tuple[str, Pipe]]): The pipeline.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  config\n",
      " |      Trainable config for the current language instance. Includes the\n",
      " |      current pipeline components, as well as default training config.\n",
      " |      \n",
      " |      RETURNS (thinc.api.Config): The config.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#config\n",
      " |  \n",
      " |  meta\n",
      " |      Custom meta data of the language class. If a model is loaded, this\n",
      " |      includes details from the model's meta.json.\n",
      " |      \n",
      " |      RETURNS (Dict[str, Any]): The meta.\n",
      " |      \n",
      " |      DOCS: https://spacy.io/api/language#meta\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from spacy.language.Language:\n",
      " |  \n",
      " |  __annotations__ = {'_factory_meta': typing.Dict[str, ForwardRef('Facto...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world.\n",
      "My name is John Moskowitz.\n",
      "I am learning Python 1 5.\n"
     ]
    }
   ],
   "source": [
    "for sent in doc1.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "Param\n",
      ",\n",
      "you\n",
      "are\n",
      "cool\n",
      "students\n"
     ]
    }
   ],
   "source": [
    "doc2 = nlp(\"I am Param, you are cool students\")\n",
    "for sentence in doc2:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "am\n",
      "Param\n",
      ",\n",
      "you\n",
      "are\n",
      "cool\n",
      "students\n"
     ]
    }
   ],
   "source": [
    "for token in doc2:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm Param, you are cool students\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(\"I'm Param, you are cool students\")\n",
    "for sentence in doc3.sents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "'m\n",
      "Param\n",
      ",\n",
      "you\n",
      "are\n",
      "cool\n",
      "students\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for sentence in doc3:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "U.S.A.\n",
      "is\n",
      "pretty\n",
      "big\n"
     ]
    }
   ],
   "source": [
    "doc4 = nlp(\" U.S.A. is pretty big\")\n",
    "for i in doc4:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc5 = nlp(\"Apple is a very small company.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S.A. GPE\n"
     ]
    }
   ],
   "source": [
    "for ent in doc4.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple ORG\n",
      "USD 1 Trillion MONEY\n",
      "California GPE\n",
      "U.S.A. GPE\n",
      "1970 DATE\n"
     ]
    }
   ],
   "source": [
    "doc6 = nlp(\"Apple is a very small company with a market cap of USD 1 Trillion. It is located in California in U.S.A. It was established in 1970.\")\n",
    "for ent in doc6.ents:\n",
    "    print(ent, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words:  326\n",
      "['only', 'whom', 'â€˜d', 'call', 'with', 'any', 'anything', 'three', 'a', 'wherein']\n"
     ]
    }
   ],
   "source": [
    "# STOPWORDS\n",
    "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
    "print(f\"Number of stop words:  {len(spacy_stopwords)}\")\n",
    "print(list(spacy_stopwords)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spacy_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"He is determined to buy a bag of apples and a bag of bananas. He was very persuasive to drop the price of each bag. He hackled with the seller.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_text = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determined\n",
      "buy\n",
      "bag\n",
      "apples\n",
      "bag\n",
      "bananas\n",
      ".\n",
      "persuasive\n",
      "drop\n",
      "price\n",
      "bag\n",
      ".\n",
      "hackled\n",
      "seller\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "tokens = [token.text for token in doc_text if not token.is_stop]\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stop_words = [\"price\"]\n",
    "for i in my_stop_words:\n",
    "    nlp.vocab[i].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "determined\n",
      "buy\n",
      "bag\n",
      "apples\n",
      "bag\n",
      "bananas\n",
      ".\n",
      "persuasive\n",
      "drop\n",
      "bag\n",
      ".\n",
      "hackled\n",
      "seller\n",
      ".\n",
      "running\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "text_custom = nlp(\"\"\"He is determined to buy a bag of apples and a bag of bananas. He was very persuasive to drop the price of each bag. He hackled with the seller. He was running the show.\"\"\")\n",
    "\n",
    "tokens = [token.text for token in text_custom if not token.is_stop]\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming and Lemmatization\n",
    "\"run\"\n",
    "\"running\"\n",
    "\"computer\"\n",
    "\"computing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_custom = nlp(\"I was eating a croissant in the park whilst by friend Peter was singing cause he got too drunk!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['eat',\n",
       " 'croissant',\n",
       " 'park',\n",
       " 'whilst',\n",
       " 'friend',\n",
       " 'Peter',\n",
       " 'singe',\n",
       " 'cause',\n",
       " 'get',\n",
       " 'drunk',\n",
       " '.']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatized = []\n",
    "for token in txt_custom:\n",
    "    if token.is_stop:\n",
    "        continue\n",
    "    lemmatized.append(token.lemma_)\n",
    "lemmatized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP False\n",
      "is be AUX VBZ True\n",
      "looking look VERB VBG False\n",
      "at at ADP IN True\n",
      "buying buy VERB VBG False\n",
      "U.K. U.K. PROPN NNP False\n",
      "startup startup NOUN NN False\n",
      "for for ADP IN True\n",
      "$ $ SYM $ False\n",
      "1 1 NUM CD False\n",
      "billion billion NUM CD False\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "58e282aa73604b14d46040a8c994c0022fdef772743121eb4f7a1644b8bf9e3d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('zeus')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
